{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "from TimeBasedCV import TimeBasedCV\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "# more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/factors_1965.csv', parse_dates=['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   permno       DATE        mvel1      beta    betasq     chmom     dolvol  \\\n",
      "0   10145 1965-02-26   1498872.00  0.983510  0.967291  0.105988  11.546907   \n",
      "1   10401 1965-02-26  35392058.00  0.780829  0.609694 -0.063768  12.240330   \n",
      "2   10786 1965-02-26   1695284.75  0.806119  0.649827 -0.130519  12.005040   \n",
      "3   10989 1965-02-26   1295887.75  1.199748  1.439395  0.073609  11.756961   \n",
      "4   11260 1965-02-26   2302001.25  1.257269  1.580725 -0.167320  12.240330   \n",
      "\n",
      "    idiovol    indmom     mom1m  ...  macro_ep  macro_bm  macro_ntis  \\\n",
      "0  0.022307  0.035075  0.104116  ...  2.936836  0.471399    0.014823   \n",
      "1  0.013395  0.335139 -0.007326  ...  2.936836  0.471399    0.014823   \n",
      "2  0.024366  0.104106  0.060498  ...  2.936836  0.471399    0.014823   \n",
      "3  0.022717  0.118513  0.068807  ...  2.936836  0.471399    0.014823   \n",
      "4  0.035883  0.185424 -0.036885  ...  2.936836  0.471399    0.014823   \n",
      "\n",
      "   macro_tbl  macro_tms  macro_dfy  macro_svar  macro_mkt-rf  macro_hml  \\\n",
      "0     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "1     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "2     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "3     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "4     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "\n",
      "   macro_smb  \n",
      "0       3.55  \n",
      "1       3.55  \n",
      "2       3.55  \n",
      "3       3.55  \n",
      "4       3.55  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# with open('data/features_1965.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "\n",
    "with open('data/features_1965.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>indmom</th>\n",
       "      <th>mom1m</th>\n",
       "      <th>...</th>\n",
       "      <th>macro_ep</th>\n",
       "      <th>macro_bm</th>\n",
       "      <th>macro_ntis</th>\n",
       "      <th>macro_tbl</th>\n",
       "      <th>macro_tms</th>\n",
       "      <th>macro_dfy</th>\n",
       "      <th>macro_svar</th>\n",
       "      <th>macro_mkt-rf</th>\n",
       "      <th>macro_hml</th>\n",
       "      <th>macro_smb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10145</td>\n",
       "      <td>1965-02-26</td>\n",
       "      <td>1498872.00</td>\n",
       "      <td>0.983510</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.105988</td>\n",
       "      <td>11.546906</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>0.104116</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936836</td>\n",
       "      <td>0.471399</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>-0.0379</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10401</td>\n",
       "      <td>1965-02-26</td>\n",
       "      <td>35392056.00</td>\n",
       "      <td>0.780829</td>\n",
       "      <td>0.609694</td>\n",
       "      <td>-0.063768</td>\n",
       "      <td>12.240331</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>0.335139</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936836</td>\n",
       "      <td>0.471399</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>-0.0379</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10786</td>\n",
       "      <td>1965-02-26</td>\n",
       "      <td>1695284.75</td>\n",
       "      <td>0.806119</td>\n",
       "      <td>0.649827</td>\n",
       "      <td>-0.130519</td>\n",
       "      <td>12.005040</td>\n",
       "      <td>0.024366</td>\n",
       "      <td>0.104106</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936836</td>\n",
       "      <td>0.471399</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>-0.0379</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10989</td>\n",
       "      <td>1965-02-26</td>\n",
       "      <td>1295887.75</td>\n",
       "      <td>1.199748</td>\n",
       "      <td>1.439395</td>\n",
       "      <td>0.073609</td>\n",
       "      <td>11.756961</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.118513</td>\n",
       "      <td>0.068807</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936836</td>\n",
       "      <td>0.471399</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>-0.0379</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11260</td>\n",
       "      <td>1965-02-26</td>\n",
       "      <td>2302001.25</td>\n",
       "      <td>1.257269</td>\n",
       "      <td>1.580725</td>\n",
       "      <td>-0.167320</td>\n",
       "      <td>12.240331</td>\n",
       "      <td>0.035883</td>\n",
       "      <td>0.185424</td>\n",
       "      <td>-0.036885</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936836</td>\n",
       "      <td>0.471399</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>-0.0379</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       DATE        mvel1      beta    betasq     chmom     dolvol  \\\n",
       "0   10145 1965-02-26   1498872.00  0.983510  0.967291  0.105988  11.546906   \n",
       "1   10401 1965-02-26  35392056.00  0.780829  0.609694 -0.063768  12.240331   \n",
       "2   10786 1965-02-26   1695284.75  0.806119  0.649827 -0.130519  12.005040   \n",
       "3   10989 1965-02-26   1295887.75  1.199748  1.439395  0.073609  11.756961   \n",
       "4   11260 1965-02-26   2302001.25  1.257269  1.580725 -0.167320  12.240331   \n",
       "\n",
       "    idiovol    indmom     mom1m  ...  macro_ep  macro_bm  macro_ntis  \\\n",
       "0  0.022307  0.035075  0.104116  ...  2.936836  0.471399    0.014823   \n",
       "1  0.013395  0.335139 -0.007326  ...  2.936836  0.471399    0.014823   \n",
       "2  0.024366  0.104106  0.060498  ...  2.936836  0.471399    0.014823   \n",
       "3  0.022717  0.118513  0.068807  ...  2.936836  0.471399    0.014823   \n",
       "4  0.035883  0.185424 -0.036885  ...  2.936836  0.471399    0.014823   \n",
       "\n",
       "   macro_tbl  macro_tms  macro_dfy  macro_svar  macro_mkt-rf  macro_hml  \\\n",
       "0     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
       "1     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
       "2     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
       "3     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
       "4     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
       "\n",
       "   macro_smb  \n",
       "0       3.55  \n",
       "1       3.55  \n",
       "2       3.55  \n",
       "3       3.55  \n",
       "4       3.55  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort observations by date and stock id\n",
    "df[df.columns[2:]] = df[df.columns[2:]].astype('float32')\n",
    "df = df.sort_values(by = ['DATE', 'permno'], ascending = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['permno2'] = df['permno'].copy()\n",
    "df['DATE2'] = df['DATE'].copy()\n",
    "\n",
    "#Make a copy of  the \"me\" variable (market equity) before rank standartization to use afterwards for value weighting\n",
    "df['mvel12'] = df['mvel1'].copy()\n",
    "df = df.set_index(['DATE2','permno2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.3 \n",
    "df_large= df.groupby('DATE').apply(lambda x: x.nlargest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n",
    "df_small = df.groupby('DATE').apply(lambda x: x.nsmallest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[~df.columns.isin(['DATE', 'DATE2', \"mvel2\",'sic2' ,'permno',\"permno2\",'risk_premium'])].tolist()\n",
    "df[features]=df.groupby('DATE')[features].rank(pct=True)\n",
    "df[features] = 2*df[features] - 1\n",
    "\n",
    "\n",
    "df_large[features]=df_large.groupby('DATE')[features].rank(pct=True)\n",
    "df_large[features] = 2*df_large[features] - 1\n",
    "\n",
    "df_small[features]=df_small.groupby('DATE')[features].rank(pct=True)\n",
    "df_small[features] = 2*df_small[features] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
    "    if in_sample:\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    else:\n",
    "        if benchmark is None:\n",
    "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - benchmark) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1965-01-31 - 1975-01-31 ,val period: 1975-01-31 - 1977-01-31 , Test period 1977-01-31 - 1978-01-31 # train records 13670 ,# val records 3499 , # test records 1941\n",
      "Train period: 1966-01-31 - 1976-01-31 ,val period: 1976-01-31 - 1978-01-31 , Test period 1978-01-31 - 1979-01-31 # train records 14434 ,# val records 3708 , # test records 2030\n",
      "Train period: 1967-01-31 - 1977-01-31 ,val period: 1977-01-31 - 1979-01-31 , Test period 1979-01-31 - 1980-01-31 # train records 15118 ,# val records 3971 , # test records 2358\n",
      "Train period: 1968-01-31 - 1978-01-31 ,val period: 1978-01-31 - 1980-01-31 , Test period 1980-01-31 - 1981-01-31 # train records 15843 ,# val records 4388 , # test records 3334\n",
      "Train period: 1969-01-31 - 1979-01-31 ,val period: 1979-01-31 - 1981-01-31 , Test period 1981-01-31 - 1982-01-31 # train records 16573 ,# val records 5692 , # test records 3578\n",
      "Train period: 1970-01-31 - 1980-01-31 ,val period: 1980-01-31 - 1982-01-31 , Test period 1982-01-31 - 1983-01-31 # train records 17432 ,# val records 6912 , # test records 2893\n",
      "Train period: 1971-01-31 - 1981-01-31 ,val period: 1981-01-31 - 1983-01-31 , Test period 1983-01-31 - 1984-01-31 # train records 19634 ,# val records 6471 , # test records 4416\n",
      "Train period: 1972-01-31 - 1982-01-31 ,val period: 1982-01-31 - 1984-01-31 , Test period 1984-01-31 - 1985-01-31 # train records 21888 ,# val records 7309 , # test records 4368\n",
      "Train period: 1973-01-31 - 1983-01-31 ,val period: 1983-01-31 - 1985-01-31 , Test period 1985-01-31 - 1986-01-31 # train records 23087 ,# val records 8784 , # test records 4870\n",
      "Train period: 1974-01-31 - 1984-01-31 ,val period: 1984-01-31 - 1986-01-31 , Test period 1986-01-31 - 1987-01-31 # train records 25581 ,# val records 9238 , # test records 6416\n",
      "Train period: 1975-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 , Test period 1987-01-31 - 1988-01-31 # train records 28417 ,# val records 11286 , # test records 6641\n",
      "Train period: 1976-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 , Test period 1988-01-31 - 1989-01-31 # train records 31555 ,# val records 13057 , # test records 5931\n",
      "Train period: 1977-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 , Test period 1989-01-31 - 1990-01-31 # train records 36204 ,# val records 12572 , # test records 6850\n",
      "Train period: 1978-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 , Test period 1990-01-31 - 1991-01-31 # train records 40904 ,# val records 12781 , # test records 6553\n",
      "Train period: 1979-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 , Test period 1991-01-31 - 1992-01-31 # train records 44805 ,# val records 13403 , # test records 7063\n",
      "Train period: 1980-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 , Test period 1992-01-31 - 1993-01-31 # train records 49297 ,# val records 13616 , # test records 8743\n",
      "Train period: 1981-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 , Test period 1993-01-31 - 1994-01-31 # train records 52516 ,# val records 15806 , # test records 8628\n",
      "Train period: 1982-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 , Test period 1994-01-31 - 1995-01-31 # train records 56001 ,# val records 17371 , # test records 10193\n",
      "Train period: 1983-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 , Test period 1995-01-31 - 1996-01-31 # train records 61851 ,# val records 18821 , # test records 11176\n",
      "Train period: 1984-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 , Test period 1996-01-31 - 1997-01-31 # train records 66063 ,# val records 21369 , # test records 12945\n",
      "Train period: 1985-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 , Test period 1997-01-31 - 1998-01-31 # train records 71888 ,# val records 24121 , # test records 16010\n",
      "Train period: 1986-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 , Test period 1998-01-31 - 1999-01-31 # train records 78194 ,# val records 28955 , # test records 15949\n",
      "Train period: 1987-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 , Test period 1999-01-31 - 2000-01-31 # train records 84723 ,# val records 31959 , # test records 14847\n",
      "Train period: 1988-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 , Test period 2000-01-31 - 2001-01-31 # train records 94092 ,# val records 30796 , # test records 18389\n",
      "Train period: 1989-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 , Test period 2001-01-31 - 2002-01-31 # train records 104110 ,# val records 33236 , # test records 16233\n",
      "Train period: 1990-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 , Test period 2002-01-31 - 2003-01-31 # train records 112107 ,# val records 34622 , # test records 15449\n",
      "Train period: 1991-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 , Test period 2003-01-31 - 2004-01-31 # train records 123943 ,# val records 31682 , # test records 17642\n",
      "Train period: 1992-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 , Test period 2004-01-31 - 2005-01-31 # train records 133113 ,# val records 33091 , # test records 17980\n",
      "Train period: 1993-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 , Test period 2005-01-31 - 2006-01-31 # train records 139819 ,# val records 35622 , # test records 21590\n",
      "Train period: 1994-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 , Test period 2006-01-31 - 2007-01-31 # train records 148833 ,# val records 39570 , # test records 23521\n",
      "Train period: 1995-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 , Test period 2007-01-31 - 2008-01-31 # train records 156620 ,# val records 45111 , # test records 24470\n",
      "Train period: 1996-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 , Test period 2008-01-31 - 2009-01-31 # train records 167034 ,# val records 47991 , # test records 21949\n",
      "Train period: 1997-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 , Test period 2009-01-31 - 2010-01-31 # train records 177610 ,# val records 46419 , # test records 16767\n",
      "Train period: 1998-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 186070 ,# val records 38716 , # test records 18170\n",
      "Train period: 1999-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 192070 ,# val records 34937 , # test records 21578\n",
      "Train period: 2000-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 193990 ,# val records 39748 , # test records 21516\n",
      "Train period: 2001-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 , Test period 2013-01-31 - 2014-01-31 # train records 193771 ,# val records 43094 , # test records 23877\n",
      "Train period: 2002-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 , Test period 2014-01-31 - 2015-01-31 # train records 199116 ,# val records 45393 , # test records 28640\n",
      "Train period: 2003-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 , Test period 2015-01-31 - 2016-01-31 # train records 205183 ,# val records 52517 , # test records 26461\n",
      "Train period: 2004-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 , Test period 2016-01-31 - 2017-01-31 # train records 211418 ,# val records 55101 , # test records 23187\n",
      "Train period: 2005-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 , Test period 2017-01-31 - 2018-01-31 # train records 222078 ,# val records 49648 , # test records 27102\n",
      "Train period: 2006-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 , Test period 2018-01-31 - 2019-01-31 # train records 226949 ,# val records 50289 , # test records 28421\n",
      "Train period: 2007-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 , Test period 2019-01-31 - 2020-01-31 # train records 226615 ,# val records 55523 , # test records 27271\n",
      "Train period: 2008-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 , Test period 2020-01-31 - 2021-01-31 # train records 229247 ,# val records 55692 , # test records 29168\n",
      "----------\n",
      "R2 1975-01-31 - 1976-12-31 training set 0.15048451317516431\n",
      "R2 1977-01-31 - 1977-12-30 test set 0.5373817427368763\n",
      "----------\n",
      "R2 1976-02-27 - 1977-12-30 training set 0.45487751307130664\n",
      "R2 1978-01-31 - 1978-12-29 test set 0.47902316401850575\n",
      "----------\n",
      "R2 1977-01-31 - 1978-12-29 training set 0.5013117871379171\n",
      "R2 1979-01-31 - 1979-12-31 test set 0.5350865734655835\n",
      "----------\n",
      "R2 1978-01-31 - 1979-12-31 training set 0.5428558114376794\n",
      "R2 1980-01-31 - 1981-01-30 test set 0.4765779487131384\n",
      "----------\n",
      "R2 1979-01-31 - 1981-01-30 training set 0.4905032873991476\n",
      "R2 1981-02-27 - 1982-01-29 test set 0.6145369756367327\n",
      "----------\n",
      "R2 1980-01-31 - 1982-01-29 training set 0.5471151328105104\n",
      "R2 1982-02-26 - 1982-12-31 test set 0.3827257384766162\n",
      "----------\n",
      "R2 1981-02-27 - 1982-12-31 training set 0.4852881602084501\n",
      "R2 1983-01-31 - 1983-12-30 test set 0.4638594728444182\n",
      "----------\n",
      "R2 1982-02-26 - 1983-12-30 training set 0.4351761334504064\n",
      "R2 1984-01-31 - 1984-12-31 test set 0.5802719772700765\n",
      "----------\n",
      "R2 1983-01-31 - 1984-12-31 training set 0.5263729298107721\n",
      "R2 1985-01-31 - 1985-12-31 test set 0.2177886941611813\n",
      "----------\n",
      "R2 1984-01-31 - 1985-12-31 training set 0.44724871398118493\n",
      "R2 1986-01-31 - 1987-01-30 test set -0.04886954989314418\n",
      "----------\n",
      "R2 1985-01-31 - 1987-01-30 training set 0.032990699941368606\n",
      "R2 1987-02-27 - 1988-01-29 test set 0.22780592813937128\n",
      "----------\n",
      "R2 1986-01-31 - 1988-01-29 training set 0.10162763904914174\n",
      "R2 1988-02-29 - 1988-12-30 test set 0.42197619554349486\n",
      "----------\n",
      "R2 1987-02-27 - 1988-12-30 training set 0.29242324161726263\n",
      "R2 1989-01-31 - 1989-12-29 test set 0.44970018167625814\n",
      "----------\n",
      "R2 1988-02-29 - 1989-12-29 training set 0.44259186020032215\n",
      "R2 1990-01-31 - 1990-12-31 test set 0.43878612246620574\n",
      "----------\n",
      "R2 1989-01-31 - 1990-12-31 training set 0.4475030981752226\n",
      "R2 1991-01-31 - 1991-12-31 test set -0.13007631676742637\n",
      "----------\n",
      "R2 1990-01-31 - 1991-12-31 training set 0.2535998963323369\n",
      "R2 1992-01-31 - 1993-01-29 test set -0.07817023937096179\n",
      "----------\n",
      "R2 1991-01-31 - 1993-01-29 training set -0.10539934058229528\n",
      "R2 1993-02-26 - 1993-12-31 test set -0.07177760336411398\n",
      "----------\n",
      "R2 1992-01-31 - 1993-12-31 training set -0.1110670378690255\n",
      "R2 1994-01-31 - 1994-12-30 test set 0.2784305635229698\n",
      "----------\n",
      "R2 1993-02-26 - 1994-12-30 training set 0.13096375543365457\n",
      "R2 1995-01-31 - 1995-12-29 test set 0.11759751882050684\n",
      "----------\n",
      "R2 1994-01-31 - 1995-12-29 training set 0.20041945982185827\n",
      "R2 1996-01-31 - 1996-12-31 test set 0.14403498505613377\n",
      "----------\n",
      "R2 1995-01-31 - 1996-12-31 training set 0.1308602071968702\n",
      "R2 1997-01-31 - 1998-01-30 test set 0.11986540388112432\n",
      "----------\n",
      "R2 1996-01-31 - 1998-01-30 training set 0.12791440993086656\n",
      "R2 1998-02-27 - 1999-01-29 test set 0.09946513776732235\n",
      "----------\n",
      "R2 1997-01-31 - 1999-01-29 training set 0.10731453583153105\n",
      "R2 1999-02-26 - 1999-12-31 test set 0.06032987252292343\n",
      "----------\n",
      "R2 1998-02-27 - 1999-12-31 training set 0.07773196378210045\n",
      "R2 2000-01-31 - 2000-12-29 test set 0.10314271960630783\n",
      "----------\n",
      "R2 1999-02-26 - 2000-12-29 training set 0.09390639080047869\n",
      "R2 2001-01-31 - 2001-12-31 test set 0.08075080263371293\n",
      "----------\n",
      "R2 2000-01-31 - 2001-12-31 training set 0.08798839410411774\n",
      "R2 2002-01-31 - 2002-12-31 test set 0.08739576202290478\n",
      "----------\n",
      "R2 2001-01-31 - 2002-12-31 training set 0.0829918083319221\n",
      "R2 2003-01-31 - 2004-01-30 test set -0.37346992682608504\n",
      "----------\n",
      "R2 2002-01-31 - 2004-01-30 training set -0.08973520266897306\n",
      "R2 2004-02-27 - 2004-12-31 test set -0.15348541402860194\n",
      "----------\n",
      "R2 2003-01-31 - 2004-12-31 training set -0.30791093818595505\n",
      "R2 2005-01-31 - 2005-12-30 test set 0.09114235326560727\n",
      "----------\n",
      "R2 2004-02-27 - 2005-12-30 training set -0.025091429555263556\n",
      "R2 2006-01-31 - 2006-12-29 test set 0.1557730585191015\n",
      "----------\n",
      "R2 2005-01-31 - 2006-12-29 training set 0.12504664630336104\n",
      "R2 2007-01-31 - 2007-12-31 test set 0.1647595856808175\n",
      "----------\n",
      "R2 2006-01-31 - 2007-12-31 training set 0.15720085841647358\n",
      "R2 2008-01-31 - 2009-01-30 test set 0.11747780956190801\n",
      "----------\n",
      "R2 2007-01-31 - 2009-01-30 training set 0.13035647682266482\n",
      "R2 2009-02-27 - 2010-01-29 test set -0.21567955181036513\n",
      "----------\n",
      "R2 2008-01-31 - 2010-01-29 training set 0.00863480076367229\n",
      "R2 2010-02-26 - 2010-12-31 test set -0.19860546351080677\n",
      "----------\n",
      "R2 2009-02-27 - 2010-12-31 training set -0.2200371448239864\n",
      "R2 2011-01-31 - 2011-12-30 test set -0.03805887157004628\n",
      "----------\n",
      "R2 2010-02-26 - 2011-12-30 training set -0.1157315250064721\n",
      "R2 2012-01-31 - 2012-12-31 test set -0.12086935897162121\n",
      "----------\n",
      "R2 2011-01-31 - 2012-12-31 training set -0.054523326612708534\n",
      "R2 2013-01-31 - 2013-12-31 test set -0.13045028620319887\n",
      "----------\n",
      "R2 2012-01-31 - 2013-12-31 training set -0.10810978207752941\n",
      "R2 2014-01-31 - 2015-01-30 test set -0.01811256208473444\n",
      "----------\n",
      "R2 2013-01-31 - 2015-01-30 training set -0.05430402752145147\n",
      "R2 2015-02-27 - 2016-01-29 test set 0.01499643584540944\n",
      "----------\n",
      "R2 2014-01-31 - 2016-01-29 training set 0.00029912845463031523\n",
      "R2 2016-02-29 - 2016-12-30 test set -0.05215238201003891\n",
      "----------\n",
      "R2 2015-02-27 - 2016-12-30 training set -0.013014625350435738\n",
      "R2 2017-01-31 - 2017-12-29 test set -0.013629872321895942\n",
      "----------\n",
      "R2 2016-02-29 - 2017-12-29 training set -0.031770563816245545\n",
      "R2 2018-01-31 - 2018-12-31 test set 0.028072557522656916\n",
      "----------\n",
      "R2 2017-01-31 - 2018-12-31 training set 0.009475180596936417\n",
      "R2 2019-01-31 - 2019-12-31 test set -0.0008103173479043768\n",
      "----------\n",
      "R2 2018-01-31 - 2019-12-31 training set -0.005497854840007976\n",
      "R2 2020-01-31 - 2021-01-29 test set -0.001010463794520522\n",
      "R2OOS Linear Regression Full:  0.029480953445106706\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=120,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'sic2', 'DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df[features]\n",
    "y = df[['risk_premium']]\n",
    "\n",
    "#Empty containers to save results from each window\n",
    "\n",
    "###########################################\n",
    "# Validation\n",
    "###########################################\n",
    "\n",
    "pred_val = []\n",
    "y_val_list =[]\n",
    "r2_val_list = []\n",
    "\n",
    "###########################################\n",
    "# Testing \n",
    "###########################################\n",
    "predictions = []\n",
    "y_test_list =[]\n",
    "dates = []\n",
    "dic_r2_all = {}\n",
    "\n",
    "# Model’s complexity: dictionary to save the number of characteristics over time\n",
    "num_coef_time = {}\n",
    "\n",
    "# List of values to use for the alpha hyperparameter\n",
    "alphas = np.linspace(start=0.00001,stop=0.004,num=20)\n",
    "# Empty container to save the objective loss function (mean squared errors) for each alpha\n",
    "mse = np.full((len(alphas),1),np.nan, dtype = np.float32)\n",
    "\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(1975,1,31), second_split_date= datetime.date(1985,1,31)):\n",
    "    print('----------') \n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "    \n",
    "    #Loop over the list containing potential alpha values, fit on the training sample and use \n",
    "    #validation set to generate predictions\n",
    "    for i in range(len(alphas)):\n",
    "        model_val = ElasticNet(alpha=alphas[i], l1_ratio=0.5)\n",
    "        model_val.fit(X_train,y_train)\n",
    "        Yval_predict = model_val.predict(X_val)\n",
    "        #calculate mean squared error for each potential value of the alpha hyperparameter\n",
    "        mse[i,0] = np.sqrt(mean_squared_error(y_val,Yval_predict))\n",
    " \n",
    "    #The optimal value of the alpha hyperparameter is the value that causes the lowest loss\n",
    "    optim_alpha = alphas[np.argmin(mse)]\n",
    "   \n",
    "    #Fit again using the train and validation set and the optimal alpha parameter\n",
    "    model = ElasticNet(alpha=optim_alpha, l1_ratio=0.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    r2_val = 1-np.sum(pow(y_val['risk_premium']-y_pred_val,2))/np.sum(pow(y_val['risk_premium'],2))\n",
    "    r2_val_list.append(r2_val)\n",
    "    print(f'R2 {y_val.index[0][0].date()} - {y_val.index[-1][0].date()} training set {r2_val}')\n",
    "\n",
    "    model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    #Use test set to generate final predictions \n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    #Save predictions, dates and the true values of the dependent variable to list  \n",
    "    predictions.append(preds)\n",
    "    dates.append(y_test.index)\n",
    "    y_test_list.append(y_test)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2 = 1-np.sum(pow(y_test['risk_premium']-preds,2))/np.sum(pow(y_test['risk_premium'],2))\n",
    "    print(f'R2 {y_test.index[0][0].date()} - {y_test.index[-1][0].date()} test set {r2}')\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all[\"r2.\" + str(y_test.index)] = r2\n",
    "    # Save the number of characteristics to inspect  model's complexity over time \n",
    "    num_coef = len(model.coef_[np.nonzero(model.coef_ != 0)])\n",
    "    num_coef_time[\"ncoef.\" + str(y_test.index)] = num_coef\n",
    "        \n",
    "\n",
    "#Calculate OOS model performance over the entire test period in line with Gu et al (2020)\n",
    "predictions_all= np.concatenate(predictions, axis=0)\n",
    "y_test_list_all= np.concatenate(y_test_list, axis=0) \n",
    "dates_all= np.concatenate(dates, axis=0)\n",
    "\n",
    "#Calculate OOS model performance over the entire test period in line with Gu et al (2020)\n",
    "R2OOS_ENet_full = r2_score(y_test_list_all, predictions_all)\n",
    "print(\"R2OOS Linear Regression Full: \", R2OOS_ENet_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1976-01-31 - 1981-01-31 ,val period: 1981-01-31 - 1983-01-31 , Test period 1983-01-31 - 1984-01-31 # train records 3405 ,# val records 1930 , # test records 1320\n",
      "Train period: 1977-01-31 - 1982-01-31 ,val period: 1982-01-31 - 1984-01-31 , Test period 1984-01-31 - 1985-01-31 # train records 3947 ,# val records 2182 , # test records 1304\n",
      "Train period: 1978-01-31 - 1983-01-31 ,val period: 1983-01-31 - 1985-01-31 , Test period 1985-01-31 - 1986-01-31 # train records 4232 ,# val records 2624 , # test records 1458\n",
      "Train period: 1979-01-31 - 1984-01-31 ,val period: 1984-01-31 - 1986-01-31 , Test period 1986-01-31 - 1987-01-31 # train records 4949 ,# val records 2762 , # test records 1919\n",
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 , Test period 1987-01-31 - 1988-01-31 # train records 5549 ,# val records 3377 , # test records 1987\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 , Test period 1988-01-31 - 1989-01-31 # train records 6012 ,# val records 3906 , # test records 1777\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 , Test period 1989-01-31 - 1990-01-31 # train records 6863 ,# val records 3764 , # test records 2049\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 , Test period 1990-01-31 - 1991-01-31 # train records 7988 ,# val records 3826 , # test records 1959\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 , Test period 1991-01-31 - 1992-01-31 # train records 8445 ,# val records 4008 , # test records 2113\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 , Test period 1992-01-31 - 1993-01-31 # train records 9190 ,# val records 4072 , # test records 2619\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 , Test period 1993-01-31 - 1994-01-31 # train records 9691 ,# val records 4732 , # test records 2583\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 , Test period 1994-01-31 - 1995-01-31 # train records 9885 ,# val records 5202 , # test records 3051\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 , Test period 1995-01-31 - 1996-01-31 # train records 10517 ,# val records 5634 , # test records 3348\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 , Test period 1996-01-31 - 1997-01-31 # train records 11323 ,# val records 6399 , # test records 3879\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 , Test period 1997-01-31 - 1998-01-31 # train records 12325 ,# val records 7227 , # test records 4797\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 , Test period 1998-01-31 - 1999-01-31 # train records 13714 ,# val records 8676 , # test records 4780\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 , Test period 1999-01-31 - 2000-01-31 # train records 15480 ,# val records 9577 , # test records 4451\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 , Test period 2000-01-31 - 2001-01-31 # train records 17658 ,# val records 9231 , # test records 5511\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 , Test period 2001-01-31 - 2002-01-31 # train records 19855 ,# val records 9962 , # test records 4865\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 , Test period 2002-01-31 - 2003-01-31 # train records 21255 ,# val records 10376 , # test records 4631\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 , Test period 2003-01-31 - 2004-01-31 # train records 23418 ,# val records 9496 , # test records 5287\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 , Test period 2004-01-31 - 2005-01-31 # train records 24404 ,# val records 9918 , # test records 5390\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 , Test period 2005-01-31 - 2006-01-31 # train records 24238 ,# val records 10677 , # test records 6472\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 , Test period 2006-01-31 - 2007-01-31 # train records 24745 ,# val records 11862 , # test records 7051\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 , Test period 2007-01-31 - 2008-01-31 # train records 25684 ,# val records 13523 , # test records 7335\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 , Test period 2008-01-31 - 2009-01-31 # train records 26645 ,# val records 14386 , # test records 6578\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 , Test period 2009-01-31 - 2010-01-31 # train records 28831 ,# val records 13913 , # test records 5023\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 31535 ,# val records 11601 , # test records 5446\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 32826 ,# val records 10469 , # test records 6469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 32459 ,# val records 11915 , # test records 6450\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 , Test period 2013-01-31 - 2014-01-31 # train records 31433 ,# val records 12919 , # test records 7158\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 , Test period 2014-01-31 - 2015-01-31 # train records 30851 ,# val records 13608 , # test records 8585\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 , Test period 2015-01-31 - 2016-01-31 # train records 29966 ,# val records 15743 , # test records 7934\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 , Test period 2016-01-31 - 2017-01-31 # train records 30546 ,# val records 16519 , # test records 6952\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 , Test period 2017-01-31 - 2018-01-31 # train records 34108 ,# val records 14886 , # test records 8126\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 , Test period 2018-01-31 - 2019-01-31 # train records 36596 ,# val records 15078 , # test records 8521\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 , Test period 2019-01-31 - 2020-01-31 # train records 37079 ,# val records 16647 , # test records 8175\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 , Test period 2020-01-31 - 2021-01-31 # train records 38755 ,# val records 16696 , # test records 8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2718929648399353"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'sic2','DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df_large[features]\n",
    "y = df_large[['risk_premium']]\n",
    "\n",
    "#Empty containers to save results from each window\n",
    "\n",
    "predictions_top = []\n",
    "y_test_list_top =[]\n",
    "dates_top = []\n",
    "dic_r2_all_top = {}\n",
    "\n",
    "alphas = np.linspace(start=0.00001,stop=0.004,num=20)\n",
    "mse = np.full((len(alphas),1),np.nan, dtype = np.float32)\n",
    "\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(1981,1,31), second_split_date= datetime.date(1991,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "\n",
    "    for i in range(len(alphas)):\n",
    "        model_val = ElasticNet(alpha=alphas[i], l1_ratio=0.5)\n",
    "        model_val.fit(X_train,y_train)\n",
    "        Yval_predict = model_val.predict(X_val)\n",
    "        mse[i,0] = np.sqrt(mean_squared_error(y_val,Yval_predict))\n",
    "\n",
    "    optim_alpha = alphas[np.argmin(mse)]\n",
    "   \n",
    "    model = ElasticNet(alpha=optim_alpha, l1_ratio=0.5)\n",
    "    model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val))) \n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    predictions_top.append(preds)\n",
    "    dates_top.append(y_test.index)\n",
    "    y_test_list_top.append(y_test)\n",
    "    \n",
    "\n",
    "    r2 = 1-np.sum(pow(y_test['risk_premium']-preds,2))/np.sum(pow(y_test['risk_premium'],2))\n",
    "    dic_r2_all_top[\"r2.\" + str(y_test.index)] = r2\n",
    "    \n",
    "predictions_all_top= np.concatenate(predictions_top, axis=0)\n",
    "y_test_list_all_top= np.concatenate(y_test_list_top, axis=0) \n",
    "dates_all_top= np.concatenate(dates_top, axis=0)\n",
    "\n",
    "R200S_ENet_Top = r2_score(y_test_list_all_top, predictions_all_top)\n",
    "R200S_ENet_Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 31535 ,# val records 11601 , # test records 5446\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 32826 ,# val records 10469 , # test records 6469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 32459 ,# val records 11915 , # test records 6450\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 , Test period 2013-01-31 - 2014-01-31 # train records 31433 ,# val records 12919 , # test records 7158\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 , Test period 2014-01-31 - 2015-01-31 # train records 30851 ,# val records 13608 , # test records 8585\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 , Test period 2015-01-31 - 2016-01-31 # train records 29966 ,# val records 15743 , # test records 7934\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 , Test period 2016-01-31 - 2017-01-31 # train records 30546 ,# val records 16519 , # test records 6952\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 , Test period 2017-01-31 - 2018-01-31 # train records 34108 ,# val records 14886 , # test records 8126\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 , Test period 2018-01-31 - 2019-01-31 # train records 36596 ,# val records 15078 , # test records 8521\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 , Test period 2019-01-31 - 2020-01-31 # train records 37079 ,# val records 16647 , # test records 8175\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 , Test period 2020-01-31 - 2021-01-31 # train records 38755 ,# val records 16696 , # test records 8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2273041009902954"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'sic2', 'DATE2', 'risk_premium'])].tolist()\n",
    "X = df_small[features]\n",
    "y = df_small[['risk_premium']]\n",
    "\n",
    "#Empty containers to save results from each window\n",
    "\n",
    "predictions_bottom = []\n",
    "y_test_list_bottom =[]\n",
    "dates_bottom = []\n",
    "dic_r2_all_bottom = {}\n",
    "\n",
    "alphas = np.linspace(start=0.00001,stop=0.004,num=20)\n",
    "mse = np.full((len(alphas),1),np.nan, dtype = np.float32)\n",
    "\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(2008,1,31), second_split_date= datetime.date(2010,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "\n",
    "    for i in range(len(alphas)):\n",
    "        model_val = ElasticNet(alpha=alphas[i], l1_ratio=0.5)\n",
    "        model_val.fit(X_train,y_train)\n",
    "        Yval_predict = model_val.predict(X_val)\n",
    "        mse[i,0] = np.sqrt(mean_squared_error(y_val,Yval_predict))\n",
    "\n",
    "    optim_alpha = alphas[np.argmin(mse)]\n",
    "   \n",
    "    model = ElasticNet(alpha=optim_alpha, l1_ratio=0.5)\n",
    "    model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val))) \n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    predictions_bottom.append(preds)\n",
    "    dates_bottom.append(y_test.index)\n",
    "    y_test_list_bottom.append(y_test)\n",
    "    \n",
    "\n",
    "    r2 = 1-sum(pow(y_test['risk_premium']-preds,2))/sum(pow(y_test['risk_premium'],2))\n",
    "    dic_r2_all_bottom[\"r2.\" + str(y_test.index)] = r2\n",
    "    \n",
    "predictions_all_bottom= np.concatenate(predictions_bottom, axis=0)\n",
    "y_test_list_all_bottom= np.concatenate(y_test_list_bottom, axis=0) \n",
    "dates_all_bottom= np.concatenate(dates_bottom, axis=0)\n",
    "\n",
    "R200S_ENet_bottom = r2_score(y_test_list_all_bottom, predictions_all_bottom)\n",
    "R200S_ENet_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_test_full = r2_score(y_test_list_all, predictions_all)\n",
    "r2_test_large = r2_score(y_test_list_all_top, predictions_all_top)\n",
    "r2_test_small = r2_score(y_test_list_all_bottom, predictions_all_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENet Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full Sample</th>\n",
       "      <td>0.016256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large Firms</th>\n",
       "      <td>0.271893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small Firms</th>\n",
       "      <td>0.227304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ENet Regression\n",
       "Full Sample         0.016256\n",
       "Large Firms         0.271893\n",
       "Small Firms         0.227304"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = np.array([[R2OOS_ENet_full],\n",
    "                  [R200S_ENet_Top],\n",
    "                  [R200S_ENet_bottom]])\n",
    "\n",
    "enet_r2 = pd.DataFrame(chart, columns=['ENet Regression'],\n",
    "                     index=['Full Sample', 'Large Firms', 'Small Firms'])\n",
    "\n",
    "enet_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_r2.to_csv(r'r2_ENet_model2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.983e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010228179501707513\n"
     ]
    }
   ],
   "source": [
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12','sic2' , 'DATE2', 'DATE', 'risk_premium'])].tolist()\n",
    "df['year'] = df['DATE'].dt.year\n",
    "\n",
    "X_train = df[features].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "y_train = df[\"risk_premium\"].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "\n",
    "X_val = df[features].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "y_val = df[\"risk_premium\"].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "\n",
    "alphas = np.linspace(start=0.00001,stop=0.004,num=20)\n",
    "mse = np.full((len(alphas),1),np.nan, dtype = np.float32)\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    model_val = ElasticNet(alpha=alphas[i], l1_ratio=0.0001)\n",
    "    model_val.fit(X_train,y_train)\n",
    "    Yval_predict = model_val.predict(X_val)\n",
    "    mse[i,0] = np.sqrt(mean_squared_error(y_val,Yval_predict))\n",
    "\n",
    "optim_alpha = alphas[np.argmin(mse)]\n",
    "model = ElasticNet(alpha=optim_alpha, l1_ratio=0.5)\n",
    "model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "preds = model.predict(np.concatenate((X_train, X_val)))\n",
    "R2OOS_all = 1-np.sum(pow(np.concatenate((y_train, y_val))-preds,2))/np.sum(pow(np.concatenate((y_train, y_val)),2))\n",
    "print(R2OOS_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in features:\n",
    "    globals()['df_' + str(j)] =  df.copy()\n",
    "    globals()['df_' + str(j)][str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.983e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.983e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.983e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.983e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.752e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\asealy\\AppData\\Local\\miniconda3\\envs\\statlearning\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+06, tolerance: 1.228e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "alphas = np.linspace(start=0.00001,stop=0.004,num=20)\n",
    "mse = np.full((len(alphas),1),np.nan, dtype = np.float32)\n",
    "    \n",
    "for j in features:\n",
    "    df_var = globals()['df_' + str(j)]\n",
    "    \n",
    "    X_train = df[features].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "    y_train = df[\"risk_premium\"].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "\n",
    "    X_val = df[features].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "    y_val = df[\"risk_premium\"].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "              \n",
    "    for i in range(len(alphas)):\n",
    "        model_val = ElasticNet(alpha=alphas[i], l1_ratio=0.0001)\n",
    "        model_val.fit(X_train,y_train)\n",
    "        Yval_predict = model_val.predict(X_val)\n",
    "        mse[i,0] = np.sqrt(mean_squared_error(y_val,Yval_predict))\n",
    "\n",
    "    optim_alpha = alphas[np.argmin(mse)]\n",
    "    model = ElasticNet(alpha=optim_alpha, l1_ratio=0.5)\n",
    "    model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    preds = model.predict(np.concatenate((X_train, X_val))) \n",
    "    \n",
    "    R2OOS_var = 1-sum(np.pow(np.concatenate((y_train, y_val))-preds,2))/np.sum(pow(np.concatenate((y_train, y_val)),2))\n",
    "    dic['R2OOS_' + str(j)] = R2OOS_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dic.items())\n",
    "imp=pd.DataFrame(dic.items(), columns=['Feature', 'R2OOS'])\n",
    "# Feature: name of the variable whose values are set to zero\n",
    "imp[\"Feature\"] = imp[\"Feature\"].str[6:]\n",
    "\n",
    "# Calculate reduction in predictive R2OOS \n",
    "imp[\"red_R2OOS\"] = R2OOS_all -imp[\"R2OOS\"]\n",
    "imp[\"var_imp\"] = imp[\"red_R2OOS\"]/np.sum(imp[\"red_R2OOS\"])\n",
    "imp=imp.sort_values(by = ['var_imp'], ascending = False)\n",
    "imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predictions_all.tolist()\n",
    "y_true = y_test_list_all.tolist()\n",
    "i = dates_all.tolist()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {'identifier': i,\n",
    "     'yhat': yhat,\n",
    "     'y_true': y_true\n",
    "    })\n",
    "\n",
    "\n",
    "results[\"identifier\"]= results[\"identifier\"].astype(\"str\")\n",
    "results[\"date\"] = results[\"identifier\"].str[12:22]\n",
    "results[\"id\"] = results[\"identifier\"].str[35:40]\n",
    "results.drop([\"identifier\"],axis = 1, inplace=True)\n",
    "results['date'] = pd.to_datetime(results['date'], format='%Y-%m-%d')\n",
    "results['MonthYear'] = results['date'].dt.to_period('M')\n",
    "results = results.sort_values(by = ['date', 'id'], ascending = True)\n",
    "results = results.set_index(['MonthYear','id'])\n",
    "results.head()\n",
    "\n",
    "# results['yhat'] = results['yhat'].apply(lambda x: x[0])\n",
    "results['y_true'] = results['y_true'].apply(lambda x: x[0])\n",
    "\n",
    "data = df[['mvel12', 'macro_tbl', 'macro_svar']].copy()\n",
    "data.reset_index(inplace=True)\n",
    "data['permno2'] = data['permno2'].astype('str')\n",
    "data['MonthYear'] = data['DATE2'].dt.to_period('M')\n",
    "data.drop('DATE2', axis=1, inplace=True)\n",
    "data.rename(columns={'permno2': 'id'}, inplace=True)\n",
    "data.rename(columns={'mvel12': 'market_cap'}, inplace=True)\n",
    "data.rename(columns={'macro_tbl': 'risk_free_rate'}, inplace=True)\n",
    "data = data.set_index(['MonthYear','id'])\n",
    "\n",
    "bigdata = pd.merge(results, data,left_index=True, right_index=True)\n",
    "bigdata.reset_index(inplace=True)\n",
    "bigdata.head()\n",
    "bigdata['returns'] = bigdata['y_true'] + bigdata['risk_free_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata.to_csv('predictions/lm_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "       391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "       404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "       417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "       430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "       443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "       456, 457], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata['MonthYear1'] = bigdata['MonthYear'].copy()\n",
    "bigdata['MonthYear'] = bigdata['MonthYear'].astype('int64')\n",
    "bigdata['NumMonth'] = bigdata['MonthYear'] - 155\n",
    "bigdata['NumMonth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bigdata['NumMonth'].unique():\n",
    "    globals()['df_' + str(i)] = bigdata[bigdata['NumMonth'] == i]\n",
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"rank\"]= globals()['df_' + str(i)]['yhat'].rank(method='first')\n",
    "    \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"DecileRank\"]=pd.qcut(globals()['df_' + str(i)]['rank'].values, 10, labels = False)\n",
    "\n",
    "#Drop normal rank, retain only decile ranks \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "     globals()['df_' + str(i)].drop('rank', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    for j,g in globals()['df_' + str(i)].groupby('DecileRank'):\n",
    "        globals()['df_' + str(i)+ \"_\" + str(j)] =  g\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)] = pd.concat([globals()['df_1_'+ str(j)], globals()['df_2_'+ str(j)]], axis=0)\n",
    "    \n",
    "# Generate 10 Dataframes for the 10 Decile portfolios 0-9: rank_9: top portfolio, rank_0: bottom portfolio\n",
    "for i in np.arange(2,457,1):\n",
    "    for j in np.arange(0,10,1):\n",
    "        globals()['rank_' + str(j)] = pd.concat([globals()['rank_' + str(j)], globals()['df_' + str(i+1)+ \"_\" + str(j)]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0, 10, 1):\n",
    "    globals()['rank_' + str(j)]['residuals'] = globals()['rank_' + str(j)]['y_true'] - globals()['rank_' + str(j)]['yhat']\n",
    "    globals()['rank_' + str(j)] = globals()['rank_' + str(j)].sort_values(['NumMonth','residuals'],\n",
    "                   ascending=[True, True]).groupby(['MonthYear'],\n",
    "                                                                  as_index=False,\n",
    "                                                                  sort=False).nth(list(range(0, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)][\"eq_weights\"] = 1/globals()['rank_' + str(j)].groupby('MonthYear')[\"id\"].transform('size')\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_ew'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['return_stock_ew'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_ew'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_ew\"].transform('sum')\n",
    "\n",
    "    globals()['monthly_rank_' + str(j)] = globals()['rank_' + str(j)][[\"MonthYear1\", \"DecileRank\",\n",
    "                                                                      \"excess_return_portfolio_ew\",\n",
    "                                                                      \"pred_excess_return_portfolio_ew\",\n",
    "                                                                      \"return_portfolio_ew\"]]\n",
    "\n",
    "    globals()['monthly_rank_' + str(j)]=globals()['monthly_rank_' + str(j)].drop_duplicates()\n",
    "\n",
    "    globals()[\"ew_mean_return_rank_\" +  str(j)]= globals()['monthly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()\n",
    "    #Time-series average of predicted excess returns\n",
    "    globals()[\"ew_mean_pred_return_rank_\" +  str(j)]= globals()['monthly_rank_' + str(j)][\"pred_excess_return_portfolio_ew\"].mean()\n",
    "    #Standard deviation of realized excess returns\n",
    "    globals()[\"std_ew_rank_\" +  str(j)]= globals()['monthly_rank_' + str(j)][\"excess_return_portfolio_ew\"].std()\n",
    "    #Annualized sharpe ratio of realized excess returns\n",
    "    globals()[\"sharpe_ew_rank_\" +  str(j)]= (globals()['monthly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()/globals()['monthly_rank_' + str(j)][\"return_portfolio_ew\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>Real</th>\n",
       "      <th>Std</th>\n",
       "      <th>Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Low (L)</th>\n",
       "      <td>-4.99%</td>\n",
       "      <td>-6.99%</td>\n",
       "      <td>6.65%</td>\n",
       "      <td>-3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.47%</td>\n",
       "      <td>-6.49%</td>\n",
       "      <td>5.78%</td>\n",
       "      <td>-3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.21%</td>\n",
       "      <td>-6.21%</td>\n",
       "      <td>5.36%</td>\n",
       "      <td>-4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.01%</td>\n",
       "      <td>-6.09%</td>\n",
       "      <td>5.34%</td>\n",
       "      <td>-3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.83%</td>\n",
       "      <td>-5.96%</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>-3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.66%</td>\n",
       "      <td>-5.92%</td>\n",
       "      <td>5.32%</td>\n",
       "      <td>-3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.49%</td>\n",
       "      <td>-5.90%</td>\n",
       "      <td>5.42%</td>\n",
       "      <td>-3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.29%</td>\n",
       "      <td>-5.94%</td>\n",
       "      <td>5.45%</td>\n",
       "      <td>-3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-3.04%</td>\n",
       "      <td>-5.90%</td>\n",
       "      <td>5.74%</td>\n",
       "      <td>-3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High (H)</th>\n",
       "      <td>-2.54%</td>\n",
       "      <td>-5.90%</td>\n",
       "      <td>6.04%</td>\n",
       "      <td>-3.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pred    Real    Std Sharpe\n",
       "Low (L)   -4.99%  -6.99%  6.65%  -3.64\n",
       "2         -4.47%  -6.49%  5.78%  -3.89\n",
       "3         -4.21%  -6.21%  5.36%  -4.01\n",
       "4         -4.01%  -6.09%  5.34%  -3.95\n",
       "5         -3.83%  -5.96%  5.31%  -3.89\n",
       "6         -3.66%  -5.92%  5.32%  -3.85\n",
       "7         -3.49%  -5.90%  5.42%  -3.77\n",
       "8         -3.29%  -5.94%  5.45%  -3.77\n",
       "9         -3.04%  -5.90%  5.74%  -3.56\n",
       "High (H)  -2.54%  -5.90%  6.04%  -3.38"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chart_np = np.array([[ew_mean_pred_return_rank_0, ew_mean_return_rank_0, std_ew_rank_0, sharpe_ew_rank_0],\n",
    "                     [ew_mean_pred_return_rank_1, ew_mean_return_rank_1, std_ew_rank_1, sharpe_ew_rank_1],\n",
    "                     [ew_mean_pred_return_rank_2, ew_mean_return_rank_2, std_ew_rank_2, sharpe_ew_rank_2],\n",
    "                     [ew_mean_pred_return_rank_3, ew_mean_return_rank_3, std_ew_rank_3, sharpe_ew_rank_3],\n",
    "                     [ew_mean_pred_return_rank_4, ew_mean_return_rank_4, std_ew_rank_4, sharpe_ew_rank_4],\n",
    "                     [ew_mean_pred_return_rank_5, ew_mean_return_rank_5, std_ew_rank_5, sharpe_ew_rank_5],\n",
    "                     [ew_mean_pred_return_rank_6, ew_mean_return_rank_6, std_ew_rank_6, sharpe_ew_rank_6],\n",
    "                     [ew_mean_pred_return_rank_7, ew_mean_return_rank_7, std_ew_rank_7, sharpe_ew_rank_7],\n",
    "                     [ew_mean_pred_return_rank_8, ew_mean_return_rank_8, std_ew_rank_8, sharpe_ew_rank_8],\n",
    "                     [ew_mean_pred_return_rank_9, ew_mean_return_rank_9, std_ew_rank_9, sharpe_ew_rank_9]])\n",
    "\n",
    "ew_df = pd.DataFrame(chart_np, columns=['Pred', 'Real', 'Std', 'Sharpe'],\n",
    "                              index=['Low (L)', '2', '3', '4', '5','6','7','8',\"9\",'High (H)'])\n",
    "ew_df['Pred'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Pred']], index = ew_df.index)\n",
    "ew_df['Real'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Real']], index = ew_df.index)\n",
    "ew_df['Std'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Std']], index = ew_df.index)\n",
    "ew_df['Sharpe'] = pd.Series([(\"%.2f\" % round(val, 2)) for val in ew_df['Sharpe']], index = ew_df.index)\n",
    "ew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='time'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(num_coef_time.items())\n",
    "numpc_table = pd.DataFrame(num_coef_time.items(), columns=['Identifier', 'num_comp'])\n",
    "numpc_table['Identifier'] = numpc_table['Identifier'].astype(str)\n",
    "numpc_table['time'] = numpc_table['Identifier'].str[20:30]\n",
    "numpc_table['time'] = pd.to_datetime(numpc_table['time'], utc=False)\n",
    "numpc_table['time'] = numpc_table['time'].dt.year\n",
    "numpc_table.drop(['Identifier'], axis = 1, inplace=True)\n",
    "\n",
    "numpc_table.set_index('time').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
